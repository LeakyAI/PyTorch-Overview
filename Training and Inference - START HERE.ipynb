{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks with PyTorch\n",
    "Copyright 2022, LEAKY.AI LLC\n",
    "\n",
    "This is Assignment 4 for the Introduction to Deep Learning with PyTorch course (www.leaky.ai). \n",
    "\n",
    "### In this assignment you will:\n",
    "- Build and train a neural network from scratch to forecast fictitious Teddy Bear sales over the course of a year using Google Colab and PyTorch\n",
    "- Compare how different loss functions, optimizers, learning rates and other parameters affects the training process\n",
    "- Track both the training loss and validation loss during the training process to ensure the model is generalizing\n",
    "- Experiment with different model types to see how they affect the final test dataset loss score\n",
    "- Plot the target output vs the actual output of the model after training in order to visualize the model’s performance\n",
    "\n",
    "### To get started:\n",
    "1.\tOpen up a web browser (preferable Chrome)\n",
    "2.\tCopy the Project GitHub Link: https://github.com/LeakyAI/PyTorch-Overview\n",
    "3.\tHead over to Google Colab (https://colab.research.google.com)\n",
    "4.\tLoad the notebook: Training and Inference - Start Here.ipynb\n",
    "5.\tReplace the [TBD]'s with your own code\n",
    "6.\tExecute the notebook after completing each cell and check your answers using the solution notebook\n",
    "\n",
    "\n",
    "Don't forget to print out and have your <b>Leaky.ai PyTorch Cheatsheet</b> handy when tackling this assignment.  You can find it on the right-hand side of the the course home landing page in the Resource section.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the PyTorch Libraries and Set Random Seeds\n",
    "Start by importing our PyTorch libraries, setting our random seed to get reproducible results and then verifying our PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import some match and visual libraries we will need\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set our seeds to get reproducible results\n",
    "torch.manual_seed(6)\n",
    "random.seed(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synthetic Teddy Bear Sales Dataset\n",
    "To begin, we will use our Teddy Bear sales data from the earlier assingment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random synthentic sales dataset for Teddy Bear sales over 1 year\n",
    "day = torch.linspace(0, 365, 500)\n",
    "teddyBears = (100*(2*torch.cos(day*4*math.pi/365)+torch.rand(500))+200)\n",
    "\n",
    "# Display our Teddy Bear sales data for year 1\n",
    "plt.scatter(day, teddyBears)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Number of Teddy Bears Sold (per day)\")\n",
    "plt.title(\"Historical Sales Data (1-Year)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Dataset\n",
    "Networks train better if inputs are small (~-1..1) so we typically scale the inputs and desired output accordingly.  Two common ways to scale:\n",
    "- Min Max Normalization – rescale values to a range of 0..1 using max and min values\n",
    "- Standardization – rescale with 0 mean and 1 std. deviation, best if outliers\n",
    "\n",
    "Since the dataset in this case does not have significant issues withoutliers, we will simply apply normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our normalized training set \n",
    "# Noramlized Value = (Input - Min) / (Max - Min)\n",
    "dayMax=day.max()\n",
    "dayMin=day.min()\n",
    "dayNormalized = (day - dayMin) / (dayMax-dayMin)\n",
    "teddyBearsMax = teddyBears.max()\n",
    "teddyBearsMin = teddyBears.min()\n",
    "teddyBearsNormalized = (teddyBears - teddyBearsMin) / (teddyBearsMax - teddyBearsMin)\n",
    "\n",
    "# Display normalized values for our training data\n",
    "plt.scatter(dayNormalized, teddyBearsNormalized)\n",
    "plt.title(\"Normalized Training Set (0..1)\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Teddy Bears Sold (per day)\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Dataset in PyTorch's Dataset and Dataloader Class\n",
    "PyTorch offers a Dataset class and a Dataloader class that speeds up the process of batching our data.  Below you will implement a PyTorch dataset using the Teddy Bear sales data above and then pass that to the DataLoader class which will automatically create our batches for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Batch Dimension\n",
    "When training, our dimensions for the input data must be [batch, day] and our output target must be [batch, teddyBearsSold].  We will use the unsqueeze function to add a dimension (dim=1) to each of our tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign input x to our normalized day values\n",
    "# Assign output y (target) to our normalized teddy bear sales\n",
    "# Use the unsqueeze tensor function to add a batch dimension to both x and y since training\n",
    "# will require the input and output to include an additional dimension specifying the batch index\n",
    "# (e.g. [day] -> [batch, day])\n",
    "x=torch.unsqueeze([TBD], dim=1)\n",
    "y=torch.unsqueeze([TBD], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data into Training, Validation and Testing Sets\n",
    "Typically you can use the 80%, 10%, 10% split rule to create the three datasets from the original data.  Lots of ways to accomplish this, here you will use a simple indexing technique to slice the data into the three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index splits for training, validation and test\n",
    "# Import numpy for shuffling index values\n",
    "import numpy as np\n",
    "\n",
    "# Start by finding the total number of items in the original dataset\n",
    "# using the len function\n",
    "total = [TBD]\n",
    "\n",
    "# Build a list of indicies and shuffle them randomly\n",
    "# Indicies should be in range of the length of x\n",
    "indices = list(range([TBD]))\n",
    "\n",
    "# Shuffle the indicies\n",
    "np.random.shuffle([TBD])\n",
    "\n",
    "# Allocate 80% of the data for the training set\n",
    "# (10% for test set and 10% for validation set)\n",
    "trainingPercent = .8\n",
    "\n",
    "# Calculate the first split point so that x[:split1] will be your training set\n",
    "split1 = int(total*trainingPercent)\n",
    "\n",
    "# Calculate your 2nd split point so that x[split1:split2] will be\n",
    "# your validation set and x[split2:] will be your testing set\n",
    "split2 = int(((total - split1)/2)+split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create our a simple dataset using the PyTorch dataset class\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, x, y):\n",
    "        # Initialize both x and y\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "        # Total number of samples in the dataset\n",
    "        return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # Return the data at location index\n",
    "        x=self.x[index]\n",
    "        y=self.y[index]\n",
    "        return x,y\n",
    "\n",
    "# Instantiate the three datasets\n",
    "train_set = Dataset(x[indices[:split1]], y[indices[:split1]])\n",
    "val_set = Dataset(x[indices[split1:split2]], y[indices[split1:split2]])\n",
    "test_set = Dataset(x[indices[split2:]], y[indices[split2:]])\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "# For the training set, make sure to set shuffle to true\n",
    "train_loader = torch.utils.data.DataLoader([TBD],\n",
    "                                           batch_size=50,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader([TBD],\n",
    "                                         batch_size=50,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader([TBD],\n",
    "                                          batch_size=50,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Train Loader - Total Number of Mini-Batches: {len(train_loader)}\")\n",
    "print (f\"Train Loader - Total Size of Dataset: {len(train_loader.sampler)}\")\n",
    "print (f\"Validation Loader - Total Number of Mini-Batches: {len(val_loader)}\")\n",
    "print (f\"Validation Loader - Total Size of Dataset: {len(val_loader.sampler)}\")\n",
    "print (f\"Test Loader - Total Number of Mini-Batches: {len(test_loader)}\")\n",
    "print (f\"Test Loader - Total Size of Dataset: {len(test_loader.sampler)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Result:  \n",
    "<pre>Train Loader - Total Number of Mini-Batches: 8\n",
    "Train Loader - Total Size of Dataset: 400\n",
    "Validation Loader - Total Number of Mini-Batches: 1\n",
    "Validation Loader - Total Size of Dataset: 50\n",
    "Test Loader - Total Number of Mini-Batches: 1\n",
    "Test Loader - Total Size of Dataset: 50</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Code and Display Model\n",
    "Since we will be training multiple times, we will create a function to train our model and a function to display the model's prediction vs. our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoreModel:  Returns to the average loss given a criterion\n",
    "#              on the entire dataset in the loader\n",
    "@torch.no_grad() \n",
    "def scoreModel(model, loader, criterion):\n",
    "    model.[TBD]                                # Set the model to inference mode\n",
    "    lossTotal = 0.0                            # Initialize the total loss\n",
    "    \n",
    "    for x,y in loader:\n",
    "        pred = [TBD]                           # Single forward pass\n",
    "        loss = criterion([TBD], [TBD])         # Calculate the average loss for the batch\n",
    "        lossTotal+=[TBD].item()*x.size(0)      # Add the average loss adjusting for size of batch\n",
    "        \n",
    "    lossAvg = [TBD]/len(loader.sampler)    # Calculate the average loss for the entire dataset\n",
    "    return lossAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Neural Network using Mini-Batches\n",
    "# Purpose:  This function will train the neural network on the\n",
    "#           given training dataset for x number of epochs\n",
    "# Inputs:\n",
    "#    epochs:     The total number of training epochs\n",
    "#    model:      The neural network definition\n",
    "#    train_loader: dataloader holding our training data\n",
    "#    val_loader:  dataloader for the validation dataset\n",
    "#    criterion:  Loss function used during training\n",
    "#    optimizer:  Algorithm for determining weights\n",
    "def train(epochs, model, train_loader, val_loader, test_loader, criterion, optimizer):\n",
    "\n",
    "    # Set the model to training mode (enable dropout, batch norm stats etc.)\n",
    "    minValLoss = float('inf')\n",
    "    model.[TBD]()\n",
    "    \n",
    "    # Train model for epochs number of epochs (full pass of training set)\n",
    "    for epoch in range([TBD]):\n",
    "        \n",
    "        # Track loss over the entire epoch\n",
    "        totalLoss = 0\n",
    "        for x, y in train_loader:\n",
    "            \n",
    "            # Perform a single forward pass with a mini-batch and calculate loss\n",
    "            y_pred = [TBD]    \n",
    "            loss = [TBD] \n",
    "            totalLoss+=[TBD]\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.[TBD]\n",
    "            loss.[TBD]\n",
    "            optimizer.[TBD]\n",
    "\n",
    "        # Calculate the Average Training Loss\n",
    "        avgTLoss = [TBD]/len(train_loader.sampler)\n",
    "\n",
    "        # Calculate validation loss and checkpoint model if lower\n",
    "        vLoss = scoreModel(model, val_loader, criterion)\n",
    "        if (minValLoss > vLoss):\n",
    "            # Save the model if the validation loss improved\n",
    "            # print (\"Model validation score improved, saving model...\")\n",
    "            torch.save(model.[TBD], \"trainingModelCheckpoint.pt\")\n",
    "            minValLoss = vLoss\n",
    "                \n",
    "        # Display average loss every 100 epochs\n",
    "        if ((epoch+1) % 100 == 0):\n",
    "            print (f\"Epoch {epoch+1}  Training Loss: {totalLoss/len(train_loader.sampler):.4f} Validation Loss: {vLoss:.4f}\")\n",
    "     \n",
    "    # Finally, score the best model on the test dataset\n",
    "    model.[TBD](torch.load(\"trainingModelCheckpoint.pt\"))\n",
    "    tLoss = scoreModel(model, test_loader, criterion)\n",
    "    print (f\"Final Average Test Dataset Loss:  {tLoss:.4f}\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model's output over the entire set of inputs\n",
    "# Compare the model's result to the actual\n",
    "def displayResults(model, x, dayMin, dayMax, teddyBearsMin, teddyBearsMax, day, teddyBears):\n",
    "    \n",
    "    # Set the model to evaluation mode (inference)\n",
    "    model.[TBD]()\n",
    "    \n",
    "    # Make a single forward pass on the inputs (x)\n",
    "    pred = [TBD]\n",
    "\n",
    "    # Re-scale our output predictions (from [0..1] to [0..500]) using\n",
    "    # teddyBearsMax and teddyBearsMin\n",
    "    predScaled = pred*(teddyBearsMax - teddyBearsMin)+teddyBearsMin\n",
    "\n",
    "    # Re-scale the input from 0..1 to 0..365 days using dayMax and dayMin\n",
    "    xScaled = x*(dayMax-dayMin)+dayMin\n",
    "    \n",
    "    # Display our model (red) vs our training data\n",
    "    plt.scatter(day, teddyBears)\n",
    "    plt.scatter(xScaled.detach().numpy()[:], predScaled.detach().numpy()[:],color=\"red\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Teddy Bears\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Model to Predict Teddy Bear Sales (Regression)\n",
    "Here, start by building a model with:\n",
    "\n",
    "- 1 input (day of the year)\n",
    "- 25 units in the first hidden layer\n",
    "- 1 output (number of teddy bears sold that day)\n",
    "\n",
    "Use a relu (F.relu) activation function for the hidden layer.  Do not use an activation function on the output as this will be a regression task (predicting the number of teddy bears sold on a given day in the year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a variable to define the number of hidden units in the hidden layer\n",
    "numHiddenUnits = 25\n",
    "\n",
    "# Build the simple Neural Network by extending the nn.Module class\n",
    "class SimpleTeddyBearModel(nn.Module):\n",
    "    \n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.[TBD](1,numHiddenUnits)\n",
    "            self.fc2 = nn.[TBD](numHiddenUnits,1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = [TBD](self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "# Create an instance of the model, save it and print out the model summary\n",
    "net = SimpleTeddyBearModel()\n",
    "torch.save(net.state_dict(), 'modelcheckpoint.pth')        \n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Result:  \n",
    "<pre>SimpleTeddyBearModel(\n",
    "  (fc1): Linear(in_features=1, out_features=25, bias=True)\n",
    "  (fc2): Linear(in_features=25, out_features=1, bias=True)\n",
    ")</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the Criterion (loss function) and Optimizer\n",
    "In order to train a neural network, we need a way to measure the loss over each iteration.  We use a loss function (criterion) to measure the output of our model vs. what the actual output we desire.  Depending on the task, you will be using a specific loss function.  In this case, the task is a regression so we will use MSELoss.  For the optimzier (algorithm used to update the weights) we will start with SGD.  Try it with a learning rate of 0.01, momentum 0.9 and nesterov set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the model weights to the same as initialized\n",
    "net.load_state_dict(torch.load('modelcheckpoint.pth'))\n",
    "\n",
    "# Select our criterion (MSE Loss) and optimzer (SGD)\n",
    "criterion = torch.nn.[TBD]\n",
    "optimizer = optim.SGD(net.parameters(), lr=[TBD], momentum=[TBD], nesterov=[TBD])\n",
    "\n",
    "# Train network for 500 epochs and display the result\n",
    "train(500, net, train_loader, val_loader, test_loader, criterion, optimizer)\n",
    "displayResults(net, x, dayMin, dayMax, teddyBearsMin, teddyBearsMax, day, teddyBears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Result:  \n",
    "<pre>Epoch 100  Training Loss: 0.0802 Validation Loss: 0.0790\n",
    "Epoch 200  Training Loss: 0.0760 Validation Loss: 0.0722\n",
    "Epoch 300  Training Loss: 0.0732 Validation Loss: 0.0675\n",
    "Epoch 400  Training Loss: 0.0713 Validation Loss: 0.0648\n",
    "Epoch 500  Training Loss: 0.0704 Validation Loss: 0.0637\n",
    "Final Average Test Dataset Loss:  0.0621\n",
    "</pre>Note: Your loss values will differ but should look similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select another Optimizer\n",
    "The optimzer specifies the algorithm used to update the weights during training.   Sometimes, other optimizers will work better depending on the dataset and model.   Try to change the optimzier to adam with a learning rate of 0.01 and compare the training results with the above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset our weights to their original values\n",
    "net.load_state_dict(torch.load('modelcheckpoint.pth'))\n",
    "\n",
    "# Try with the Adam optimizer and a learning rate of 0.01\n",
    "# Use the same criterion as above (MSE Loss)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=[TBD])\n",
    "\n",
    "# Train network and display result\n",
    "train(500, net, train_loader, val_loader, test_loader, criterion, optimizer)\n",
    "displayResults(net, x, dayMin, dayMax, teddyBearsMin, teddyBearsMax, day, teddyBears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Result:  \n",
    "<pre>Epoch 100  Training Loss: 0.0659 Validation Loss: 0.0542\n",
    "Epoch 200  Training Loss: 0.0644 Validation Loss: 0.0513\n",
    "Epoch 300  Training Loss: 0.0626 Validation Loss: 0.0492\n",
    "Epoch 400  Training Loss: 0.0636 Validation Loss: 0.0489\n",
    "Epoch 500  Training Loss: 0.0078 Validation Loss: 0.0090\n",
    "Final Average Test Dataset Loss:  0.0095</pre>Note: Your loss values will differ but should look similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Different Models and Training Parameters\n",
    "Now, explore different learning rates, models and optimizers.  Start by adjusting one slightly, then another and so on.  For example, what happens when you reduce the model's hidden units below 10?  What if you add another hidden layer?   Are you able to make the predicted (red) curve fit better by adjusting other values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a variable to define the number of hidden units in the hidden layer\n",
    "numHiddenUnits = [TBD]\n",
    "\n",
    "class MyOwnModel(nn.Module):\n",
    "    \n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            [TBD]\n",
    "        \n",
    "        def forward(self, x):\n",
    "            [TBD]\n",
    "            return x\n",
    "\n",
    "# Create an instance of the model and print out a model summary\n",
    "net2 = MyOwnModel()\n",
    "\n",
    "# Setup your training optimizer and specify your loss criteria\n",
    "optimizer = [TBD]\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Train model using mini-batches and print out result\n",
    "train(500, net, train_loader, val_loader, test_loader, criterion, optimizer)\n",
    "displayResults(net2, x, dayMin, dayMax, teddyBearsMin, teddyBearsMax, day, teddyBears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- You built a PyTorch deep neural network\n",
    "- You trained the network and compared the ouput with the training set\n",
    "- You experimented with different optimizers, learning rates and epochs\n",
    "\n",
    "Congratulations on finishing the assingment!  Keep it going!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
