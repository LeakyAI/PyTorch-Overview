{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Cars - MPG Prediction (Regression)\n",
    "Copyright 2023, LEAKY.AI LLC\n",
    "\n",
    "This is project 2 for the Introduction to Deep Learning with PyTorch course (https://www.leaky.ai).\n",
    "\n",
    "In this project you will build a neural network from scratch and train it to automatically predict Miles per Gallon (MPG) for various types of cars by simply looking at other properties of the car. You will be working with a real-world car dataset which will include values for each car including the number of cylinders, horsepower, weight, model year and acceleration.  From these values, the goal of your neural network will be to predict the expected Mile per Gallon (MPG for short) of the car.\n",
    "\n",
    "### In this project you will:\n",
    "- Build and train a neural network from scratch to predict MPG (Miles per Gallon) for cars using other car attributes (year, model, displacement etc.)\n",
    "- Prepare a real-world dataset of car attributes (along with MPGs) to be used for training the neural network\n",
    "- Experiment with different models to achieve a target average loss (or better) on the test set\n",
    "- Use the trained model to make new predictions of MPGs\n",
    "\n",
    "### To get started:\n",
    "- Open up a web browser (preferable Chrome)\n",
    "- Copy the Project GitHub Link: https://github.com/LeakyAI/PyTorch-Overview\n",
    "- Head over to Google Colab (https://colab.research.google.com)\n",
    "- Load the notebook: Project Cars - START HERE.ipynb\n",
    "- Replace the [TBD]'s with your own code\n",
    "- Execute the notebook after completing each cell\n",
    "\n",
    "### Hint\n",
    "Don't forget to print out and have your PyTorch and Pandas Cheatsheet handy when tackling this project. You can find it on the right-hand side of the the course home landing page in the Resource section.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "He-tOgRZMq9l"
   },
   "outputs": [],
   "source": [
    "# Import PyTorch libraries\n",
    "import [TBD]\n",
    "import [TBD] as nn\n",
    "import [TBD] as optim\n",
    "import [TBD] as F\n",
    "\n",
    "# Import math and visual libraries we will need\n",
    "import math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set our seeds to get reproducible results\n",
    "torch.manual_seed(4)\n",
    "random.seed(4)\n",
    "\n",
    "# Modify print options for numpy and pandas\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Analyze the Dataset\n",
    "Below you will download the car dataset and explore the different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD0aBccEMx3c"
   },
   "outputs": [],
   "source": [
    "# Dataset from UCI - https://archive.ics.uci.edu/ml/datasets/auto+mpg\n",
    "# Dua, D. and Graff, C. (2019). UCI Machine Learning Repository Irvine, CA\n",
    "# University of California, School of Information and Computer Science.\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "cars = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', \n",
    "                   sep=' ', skipinitialspace=True)\n",
    "cars['Origin']=cars['Origin'].replace({1:'USA',2:'Europe',3:'Japan'})\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5CQZr5pOMzHt",
    "outputId": "2f7f6552-5f40-4342-e781-df13003f5f85"
   },
   "outputs": [],
   "source": [
    "cars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:  What are some of the common dataset issues you see with the above tables that will need to tackled before we can use it for training neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer:   [TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare our Dataset\n",
    "### Start by counting and removing rows with missing items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of missing items from each attribute\n",
    "cars.[TBD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffOKOBu5M8BL"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing items\n",
    "cars = cars.[TBD]\n",
    "\n",
    "# Check all missing items have been removed\n",
    "cars.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode categorical attributes\n",
    "Origin is a categorical attribute in this dataset.  Start by determining how many categories exits after dropping any missing items from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "yTc3Q3M2NOA-",
    "outputId": "78df25d3-4825-4859-990c-47c649917d1a"
   },
   "outputs": [],
   "source": [
    "# Count number of unique values in each attribute including 'Origin'\n",
    "cars.[TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:  How many unique categories exist for Origin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer:  [TBD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot encode the Origin attribute using pd.get_dummies, drop and pd.concat\n",
    "oneHot = pd.get_dummies([TBD])\n",
    "cars=cars.drop([TBD],axis=1)\n",
    "cars = pd.concat([TBD],axis=1)\n",
    "cars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many total attributes are left in the dataset after one hot encoding the Origin attribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer:  [TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract input X and output Y (MPG) from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract our target output ydf (MPG) and input x attributes\n",
    "ydf = [TBD]\n",
    "xdf = cars.drop([TBD],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply standardization to our input xdf and output ydf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and save values required values to standardize both the input xdf and ydf\n",
    "xMean = xdf.[TBD]\n",
    "xStd = xdf.[TBD]\n",
    "yMean = ydf.[TBD]\n",
    "yStd = ydf.[TBD]\n",
    "\n",
    "# Standardize both the input (xdf) and output (ydf) values\n",
    "xdf = [TBD]\n",
    "ydf = [TBD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the input xdf values are now standardized\n",
    "xdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:  What are you checking in the above to ensure the input values are correctly standardized?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer: [TBD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the output target ydf (MPG) is also correctly standardized\n",
    "ydf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataframes to PyTorch Tensors and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and output PyTorch tensors of type torch.float\n",
    "x = torch.tensor([TBD],dtype=[TBD])\n",
    "y = torch.tensor([TBD],dtype=[TBD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a batch dimension to y and print out the shape and type of both x and y\n",
    "y = y.unsqueeze(dim=1)\n",
    "print (f\"x.shape -> {x.shape}\")\n",
    "print (f\"x.type() -> {x.type()}\")\n",
    "print (f\"y.shape -> {y.shape}\")\n",
    "print (f\"y.type() -> {y.type()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index splits for training, validation and test\n",
    "# Import numpy for shuffling index values\n",
    "import numpy as np\n",
    "\n",
    "# Start by finding the total number of items in the original dataset\n",
    "# using the len function\n",
    "total = len(x)\n",
    "\n",
    "# Build a list of indicies and shuffle them randomly\n",
    "# Indicies should be in range of the length of x\n",
    "indices = list(range(total))\n",
    "\n",
    "# Shuffle the indicies\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Allocate 80% of the data for the training set\n",
    "# (10% for test set and 10% for validation set)\n",
    "trainingPercent = .8\n",
    "\n",
    "# Calculate the first split point so that x[:split1] will be your training set\n",
    "split1 = int(total*trainingPercent)\n",
    "\n",
    "# Calculate your 2nd split point so that x[split1:split2] will be\n",
    "# your validation set and x[split2:] will be your testing set\n",
    "split2 = int(((total - split1)/2)+split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our a simple dataset using the PyTorch dataset class\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, x, y):\n",
    "        # Initialize both x and y\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "        # Total number of samples in the dataset\n",
    "        return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # Return the data at location index\n",
    "        x=self.x[index]\n",
    "        y=self.y[index]\n",
    "        return x,y\n",
    "\n",
    "# Instantiate the three datasets\n",
    "train_set = Dataset(x[indices[:split1]], y[indices[:split1]])\n",
    "val_set = Dataset(x[indices[split1:split2]], y[indices[split1:split2]])\n",
    "test_set = Dataset(x[indices[split2:]], y[indices[split2:]])\n",
    "\n",
    "# Create dataloaders for each dataset\n",
    "# For the training set, make sure to set shuffle to true\n",
    "train_loader = torch.utils.data.DataLoader([TBD])\n",
    "val_loader = torch.utils.data.DataLoader([TBD])\n",
    "test_loader = torch.utils.data.DataLoader([TBD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size and number of batches for each set\n",
    "print (f\"Train Loader - Total Number of Mini-Batches: {len(train_loader)}\")\n",
    "print (f\"Train Loader - Total Size of Dataset: {len(train_loader.sampler)}\")\n",
    "print (f\"Validation Loader - Total Number of Mini-Batches: {len(val_loader)}\")\n",
    "print (f\"Validation Loader - Total Size of Dataset: {len(val_loader.sampler)}\")\n",
    "print (f\"Test Loader - Total Number of Mini-Batches: {len(test_loader)}\")\n",
    "print (f\"Test Loader - Total Size of Dataset: {len(test_loader.sampler)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a scoring function for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose:  This function will calculate the average loss\n",
    "#           using the criterion on the loader dataset\n",
    "# Returns:  Average loss\n",
    "@torch.[TBD] \n",
    "def scoreModel(model, loader, criterion):\n",
    "    model.[TBD]                                # Set the model to inference mode\n",
    "    lossTotal = 0.0                            # Initialize the total loss\n",
    "    \n",
    "    for x,y in loader:\n",
    "        pred = [TBD]                           # Single forward pass\n",
    "        loss = [TBD]                           # Calculate the average loss for the batch\n",
    "        lossTotal+=loss.item()*x.size(0)       # Add the average loss adjusting for size of batch\n",
    "        \n",
    "    lossAvg = lossTotal/len(loader.sampler)    # Calculate the average loss for the entire dataset\n",
    "    return lossAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose:  This function will train the neural network on the\n",
    "#           given training dataset for epochs number of iterations\n",
    "#           returning the average loss on the test set\n",
    "# Inputs:\n",
    "#    epochs:       The total number of training epochs\n",
    "#    model:        The neural network definition\n",
    "#    train_loader: Vraining set dataloader \n",
    "#    val_loader:   Validation set dataloader \n",
    "#    test_loader:  Testing set dataloader\n",
    "#    criterion:    Loss function used during training\n",
    "#    optimizer:    Algorithm for determining weights\n",
    "#\n",
    "# Returns:\n",
    "#    tLoss:        Average loss on the test_loader\n",
    "def train(epochs, model, train_loader, val_loader, test_loader, criterion, optimizer):\n",
    "\n",
    "    # Set the model to training mode (enable dropout, batch norm stats etc.)\n",
    "    minValLoss = float('inf')\n",
    "    model.[TBD]\n",
    "    \n",
    "    # Train model for epochs number of epochs (full pass of training set)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Track loss over the entire epoch\n",
    "        totalLoss = 0\n",
    "        for x, y in train_loader:\n",
    "            \n",
    "            # Perform a single forward pass with a mini-batch and calculate loss\n",
    "            y_pred = [TBD] \n",
    "            loss = [TBD]\n",
    "            totalLoss+=loss.item()*x.size(0)\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.[TBD]\n",
    "            loss.[TBD]\n",
    "            optimizer.[TBD]\n",
    "\n",
    "        # Calculate the Average Training Loss\n",
    "        avgTLoss = totalLoss/len(train_loader.sampler)\n",
    "\n",
    "        # Calculate validation loss and checkpoint model if lower\n",
    "        vLoss = scoreModel([TBD],[TBD],[TBD])\n",
    "        if (minValLoss > vLoss):\n",
    "            # Save the model if the validation loss improved\n",
    "            # print (\"Model validation score improved, saving model...\")\n",
    "            torch.save(model.state_dict(), \"trainingModelCheckpoint.pt\")\n",
    "            minValLoss = vLoss\n",
    "                \n",
    "        # Display average loss every 50 epochs\n",
    "        if ((epoch+1) % 50 == 0):\n",
    "            print (f\"Epoch {epoch+1}  Training Loss: {totalLoss/len(train_loader.sampler):.4f} Validation Loss: {vLoss:.4f}\")\n",
    "     \n",
    "    # Finally, score the best model on the test dataset\n",
    "    model.load_state_dict(torch.load(\"trainingModelCheckpoint.pt\"))\n",
    "    tLoss = scoreModel([TBD], [TBD], [TBD])\n",
    "    print (f\"Final Average Test Dataset Loss:  {tLoss:.4f}\")  \n",
    "    \n",
    "    # Return the average loss on the test set\n",
    "    return [TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a graphing function to visualize your final results on the test set\n",
    "Since your model is a regression, one way to visualize it's performance is to plot it's predictions vs. the actual predictions on the test set using a scatter plot.  That way you can easily see how close the model's predicted values were compared with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs. true values\n",
    "@torch.no_grad() \n",
    "def graphPredictions(model, loader, minValue, maxValue):\n",
    "    \n",
    "    model.eval()                               # Set the model to inference mode\n",
    "    \n",
    "    predictions=[]                             # Track predictions\n",
    "    actual=[]                                  # Track the actual labels\n",
    "    \n",
    "    for x,y in loader:\n",
    "        \n",
    "        # Single forward pass\n",
    "        pred = model(x)                               \n",
    "\n",
    "        # Un-normalize our prediction and label\n",
    "        pred = pred*yStd+yMean                 \n",
    "        y= y*yStd+yMean\n",
    "\n",
    "        # Save prediction and actual label\n",
    "        predictions.append([TBD].tolist())\n",
    "        actual.append([TBD].tolist())\n",
    "    \n",
    "    # Plot actuals vs predictions\n",
    "    plt.scatter(actual, predictions)\n",
    "    plt.xlabel('Actual MPGs')\n",
    "    plt.ylabel('Predicted MPGs')\n",
    "    plt.plot([minValue,maxValue], [minValue,maxValue]) \n",
    "    plt.xlim(minValue, maxValue)\n",
    "    plt.ylim(minValue, maxValue)\n",
    " \n",
    "    # Make the display equal in both dimensions\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train your Model - Target an Average Test Loss < 0.2\n",
    "Build your model and train it to achieve an average test loss of less than 0.2 by:\n",
    "\n",
    "- Iterating on the model size and shape\n",
    "- Experiementing with different optimziers and learning rates\n",
    "- Adding regularization techniques like dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1 - Create a model and save the initialized weights\n",
    "# Use a variable to define the number of hidden units in the hidden layer\n",
    "numHiddenUnits = [TBD]\n",
    "\n",
    "# Build the simple Neural Network by extending the nn.Module class\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            [TBD]\n",
    "            \n",
    "        def forward(self, x):\n",
    "            [TBD]\n",
    "            return x\n",
    "\n",
    "# Create an instance of the model, save it and print out the model summary\n",
    "net = MyModel()\n",
    "torch.save(net.state_dict(), 'modelcheckpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2 - Reset the model and train using different learning rates and optimziers\n",
    "# in order to acheive a test loss < 0.2\n",
    "\n",
    "# Load the model weights to the same as initialized\n",
    "net.load_state_dict(torch.load('modelcheckpoint.pth'))\n",
    "\n",
    "# Select our criterion (MSE Loss) and optimzer (SGD or Adam or experiment with others...)\n",
    "criterion = [TBD]\n",
    "optimizer = [TBD]\n",
    "\n",
    "# Train network for 1000 epochs (experiment with more or less as well)\n",
    "train([TBD], [TBD], [TBD], [TBD], [TBD], [TBD], [TBD])\n",
    "\n",
    "# Display the final results on the test set\n",
    "graphPredictions(net, test_loader, 0, 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make your own predictions\n",
    "After you achieve an average test loss of less than 0.2, you can use the model to make your own predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your inputs to your model\n",
    "df = pd.DataFrame()\n",
    "df.loc[0,'Cylinders']=[TBD]\n",
    "df.loc[0,'Displacement']=[TBD]\n",
    "df.loc[0,'Horsepower']=[TBD]\n",
    "df.loc[0,'Weight']=[TBD]\n",
    "df.loc[0,'Acceleration']=[TBD]\n",
    "df.loc[0,'Model Year']=[TBD]\n",
    "df.loc[0,'Origin_Japan']=[TBD]\n",
    "df.loc[0,'Origin_USA']=[TBD]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input values using the previously calculate xMean and sStd\n",
    "xdf = (df-[TBD])/[TBD]\n",
    "xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch tensor for the input\n",
    "x = torch.tensor(xdf.to_numpy(), dtype=torch.float)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "net = net.[TBD]\n",
    "\n",
    "# Perform a single forward pass\n",
    "pred = net(x)\n",
    "\n",
    "# Unnormalize the output using yMean and yStd\n",
    "pred = pred*yStd+yMean\n",
    "\n",
    "print (f\"Predicted MPG: {pred.item():0.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Your Project \n",
    "You have made it to the end of the project!  Once you have achieved a training set average loss of less than 0.2, you may submit your project by downloading it and emailing it to us for review.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "project 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
